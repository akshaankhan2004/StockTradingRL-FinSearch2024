{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nifty50_index_symbol = '^NSEI'\n",
    "\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2019-06-30'\n",
    "\n",
    "data = yf.download(nifty50_index_symbol, start=start_date, end=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisation(col):\n",
    "    new_col = (col - min(col))/(max(col) - min(col)) + 0.01  # for non zero stock prices\n",
    "    return new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    df[col] = normalisation(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arr = df[:600].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arr = data_arr.reshape((data_arr.shape[0], ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_arr = data_arr[:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockMarketEnvironment:\n",
    "    # action == 0-buy, 1-sell, 2-hold\n",
    "    def __init__(self, starting_balance, data):\n",
    "        self.data = data\n",
    "        self.starting_balance = starting_balance\n",
    "        self.balance = starting_balance\n",
    "        self.stocks = 0\n",
    "        self.portfolio = self.balance\n",
    "        self.time_step = 0\n",
    "        self.end = False\n",
    "        self.state = np.array([self.balance, self.stocks, self.data[self.time_step]])\n",
    "    \n",
    "    def reset(self):\n",
    "        self.balance = self.starting_balance\n",
    "        self.stocks = 0\n",
    "        self.portfolio = self.balance\n",
    "        self.time_step = 0\n",
    "        self.end = False\n",
    "        self.state = np.array([self.balance, self.stocks, self.data[self.time_step]])\n",
    "\n",
    "    def step(self, action):\n",
    "        curr_state = self.state\n",
    "        if action < 0:\n",
    "            num_poss_buy = self.balance / curr_state[2]\n",
    "            buy = num_poss_buy * abs(action)\n",
    "            self.stocks += buy\n",
    "            self.balance -= buy * curr_state[2]\n",
    "        if action > 0:\n",
    "            num_poss_sell = self.stocks\n",
    "            sell = num_poss_sell * action\n",
    "            self.stocks -= sell\n",
    "            self.balance += sell * curr_state[2]\n",
    "            \n",
    "        next_time_state = self.time_step + 1\n",
    "        if next_time_state >= self.data.shape[0] - 1:\n",
    "            self.end = True\n",
    "        next_state = np.array([self.balance, self.stocks, self.data[next_time_state]])\n",
    "\n",
    "        self.portfolio = self.balance + self.stocks * curr_state[2]\n",
    "        next_portfolio = self.balance + self.stocks * next_state[2]\n",
    "\n",
    "        reward = next_portfolio - self.portfolio\n",
    "\n",
    "        self.state = next_state\n",
    "        self.portfolio = next_portfolio\n",
    "        self.time_step += 1\n",
    "\n",
    "        return (self.state, reward, self.end)\n",
    "    \n",
    "    def is_end(self):\n",
    "        return self.end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    inputs = tf.keras.layers.Input(shape = (3, ))\n",
    "    out = tf.keras.layers.Dense(32, activation = 'relu')(inputs)\n",
    "    out = tf.keras.layers.Dense(64, activation = 'relu')(out)\n",
    "    out = tf.keras.layers.Dense(64, activation = 'relu')(out)\n",
    "    out = tf.keras.layers.Dense(32, activation = 'relu')(out)\n",
    "    outputs = tf.keras.layers.Dense(1, activation = 'tanh')(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_critic():\n",
    "    state_input = tf.keras.layers.Input(shape = (3, ))\n",
    "    out = tf.keras.layers.Dense(32, activation = 'relu')(state_input)\n",
    "    state_out = tf.keras.layers.Dense(64, activation = 'relu')(out)\n",
    "    actor_input = tf.keras.layers.Input(shape = (1, ))\n",
    "    actor_out = tf.keras.layers.Dense(64, activation = 'relu')(actor_input)\n",
    "    concat = tf.keras.layers.Concatenate()([state_out, actor_out])\n",
    "    total_out = tf.keras.layers.Dense(256, activation = 'relu')(concat)\n",
    "    total_out = tf.keras.layers.Dense(256, activation = 'relu')(total_out)\n",
    "    outputs = tf.keras.layers.Dense(1)(total_out)\n",
    "    model = tf.keras.Model([state_input, actor_input], outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_model = get_actor()\n",
    "target_actor = copy(actor_model)\n",
    "critic_model = get_critic()\n",
    "target_critic = copy(critic_model)\n",
    "\n",
    "actor_optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "critic_optimizer = tf.keras.optimizers.Adam(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1\n",
    "epsilon_decay = 0.95\n",
    "buffer_size = 400\n",
    "buffer = []\n",
    "max_iters = 100\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance = np.mean(data_arr)\n",
    "env = StockMarketEnvironment(balance, data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(env.state).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = target_actor.predict(curr_state.reshape((-1, 3)), verbose = 0)\n",
    "action[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_loss_history = []\n",
    "actor_loss_history = []\n",
    "rewards_history = []\n",
    "for i in range(max_iters + 400):\n",
    "    action_count = []\n",
    "    env.reset()\n",
    "    rewards_sum = 0\n",
    "    while not env.is_end():\n",
    "        curr_state = env.state\n",
    "        random_num = np.random.rand()\n",
    "        if random_num < epsilon:\n",
    "            action = np.random.uniform(-1, 1)\n",
    "        else:\n",
    "            action = target_actor.predict(curr_state.reshape((-1, 3)), verbose = 0)\n",
    "            action = action.item()\n",
    "        action_count.append(action)\n",
    "        next_state, reward, end = env.step(action)\n",
    "        rewards_sum += reward\n",
    "\n",
    "        quadruple = (curr_state, action, reward, next_state, end)\n",
    "        buffer.append(quadruple)\n",
    "        if len(buffer) > buffer_size:\n",
    "            buffer.pop(0)\n",
    "\n",
    "    rewards_history.append(rewards_sum)\n",
    "    \n",
    "    sample_indices = np.random.choice(len(buffer), size=min(50, len(buffer)), replace=False)\n",
    "    train_sample = [buffer[idx] for idx in sample_indices]\n",
    "    epsilon = max(epsilon * epsilon_decay, 0.1)\n",
    "\n",
    "    total_critic_loss = 0\n",
    "    total_actor_loss = 0\n",
    "    for quadrup in train_sample:\n",
    "        curr_state, action, reward, next_state, end = quadrup\n",
    "\n",
    "        # critic taining\n",
    "        with tf.GradientTape() as tape:\n",
    "            next_state_action = target_actor(tf.convert_to_tensor(next_state.reshape((-1, 3))), training = True)\n",
    "            next_state_q_vals = target_critic([tf.convert_to_tensor(next_state.reshape((-1, 3))), next_state_action], training = True)\n",
    "            target_q_val = reward + gamma * next_state_q_vals\n",
    "            curr_q_val = critic_model([tf.convert_to_tensor(curr_state.reshape((-1, 3))), tf.convert_to_tensor(np.array(action).reshape((-1, 1)))], training = True)\n",
    "            critic_loss = tf.square(target_q_val - curr_q_val)\n",
    "        critic_grads = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(zip(critic_grads, critic_model.trainable_variables))\n",
    "        total_critic_loss += critic_loss\n",
    "\n",
    "        #actor training\n",
    "        with tf.GradientTape() as tape:\n",
    "            actor_curr_act = actor_model(tf.convert_to_tensor(curr_state.reshape((-1, 3))), training = True)\n",
    "            actor_curr_q_val = critic_model([tf.convert_to_tensor(curr_state.reshape((-1, 3))), actor_curr_act], training = True)\n",
    "            actor_loss = -1 * actor_curr_q_val\n",
    "        actor_grads = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(zip(actor_grads, actor_model.trainable_variables))\n",
    "        total_actor_loss += actor_loss\n",
    "\n",
    "    \n",
    "    total_critic_loss /= len(train_sample)\n",
    "    critic_loss_history.append(total_critic_loss)\n",
    "    total_actor_loss /= len(train_sample)\n",
    "    actor_loss_history.append(total_actor_loss)\n",
    "\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_actor.save_weights(\"actor_trained.weights.h5\")\n",
    "    target_critic.save_weights(\"critic_trained.weights.h5\")\n",
    "\n",
    "    print(i, \"-> CRITIC LOSS:\", total_critic_loss.numpy(), \"-> ACTOR LOSS:\", total_actor_loss.numpy(), \"-> REWARD:\", rewards_sum, \"\\n-> Action count:\", action_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_actor.save_weights(\"best_actor_trained.weights.h5\")\n",
    "target_critic.save_weights(\"best_critic_trained.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(critic_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_count = []\n",
    "env.reset()\n",
    "rewards_sum = 0\n",
    "stock_history = []\n",
    "while not env.is_end():\n",
    "    curr_state = env.state\n",
    "    action = target_actor.predict(curr_state.reshape((-1, 3)), verbose = 0)\n",
    "    action = action.numpy().item()\n",
    "    action_count.append(action)\n",
    "    next_state, reward, end = env.step(action.numpy().item())\n",
    "    rewards_sum += reward\n",
    "    stock_history.append(env.stocks)\n",
    "print(rewards_sum)\n",
    "print(stock_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.starting_balance)\n",
    "print(env.portfolio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
