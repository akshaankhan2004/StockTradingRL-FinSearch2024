{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from copy import copy\n",
    "from copy import deepcopy\n",
    "import talib\n",
    "from gym_trading_env.renderer import Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nifty50_index_symbol = '^NSEI'\n",
    "\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2019-06-30'\n",
    "# end_date = '2012-06-11'\n",
    "\n",
    "data = yf.download(nifty50_index_symbol, start=start_date, end=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['feature-MACD'], df['feature-MACD_signal'], df['feature-MACD_hist'] = talib.MACD(df['close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "\n",
    "df['feature-RSI'] = talib.RSI(df['close'], timeperiod=14)\n",
    "\n",
    "df['feature-CCI'] = talib.CCI(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "\n",
    "df['feature-ADX'] = talib.ADX(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "\n",
    "df.dropna(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = np.linspace(0, 1, num=11).tolist()\n",
    "\n",
    "for i in range(len(action_space)):\n",
    "    action_space[i] = round(action_space[i], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"TradingEnv\",\n",
    "        name= \"RL\",\n",
    "        df = df,\n",
    "        positions =action_space , # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "        # trading_fees = 0.01/100, # 0.01% per stock buy / sell\n",
    "        # borrow_interest_rate= 0.0003/100, # 0.0003% per timestep \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(action_space[6], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        out = torch.relu(self.fc1(state))\n",
    "        out = torch.relu(self.fc2(out))\n",
    "        output = torch.tanh(self.out(out))\n",
    "        return output\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Critic, self).__init__()\n",
    "        self.actor_fc1 = nn.Linear(1, 64)\n",
    "        self.actor_fc2 = nn.Linear(64, 128)\n",
    "        self.critic_fc1 = nn.Linear(input_shape, 64)\n",
    "        self.critic_fc2 = nn.Linear(64, 128)\n",
    "        self.concatenated_1 = nn.Linear(256, 512)\n",
    "        self.concatenated_2 = nn.Linear(512, 512)\n",
    "        self.output = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, actor_input, state_input):\n",
    "        ac_out = torch.relu(self.actor_fc1(actor_input))\n",
    "        ac_out = torch.relu(self.actor_fc2(ac_out))\n",
    "\n",
    "        cri_out = torch.relu(self.critic_fc1(state_input))\n",
    "        cri_out = torch.relu(self.critic_fc2(cri_out))\n",
    "\n",
    "        concat = torch.cat([cri_out, ac_out], dim=-1)\n",
    "        out = torch.relu(self.concatenated_1(concat))\n",
    "        out = torch.relu(self.concatenated_2(out))\n",
    "        output = self.output(out)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_model = Actor(input_shape)\n",
    "target_actor = deepcopy(actor_model)\n",
    "critic_model = Critic(input_shape)\n",
    "target_critic = deepcopy(critic_model)\n",
    "\n",
    "actor_optimizer = torch.optim.Adam(list(actor_model.parameters()), lr = 0.01)\n",
    "critic_optimizer = torch.optim.Adam(list(critic_model.parameters()), lr = 0.01)\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 1000\n",
    "buffer = []\n",
    "max_iters = 10000\n",
    "gamma = 0.95\n",
    "epsilon = 1\n",
    "eps_decay = 0.995\n",
    "min_eps = 0.05\n",
    "tau = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_state, info = env.reset()\n",
    "env.step(5)\n",
    "int(target_actor(torch.Tensor(curr_state)).item()*10)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target_net, source_net, tau):\n",
    "    for target_param, source_param in zip(target_net.parameters(), source_net.parameters()):\n",
    "        target_param.data.copy_(tau * source_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# critic_state_dict = torch.load('ddpg_critic_model_weights.pth', weights_only=True)\n",
    "# actor_state_dict = torch.load('ddpg_actor_model_weights.pth', weights_only=True)\n",
    "\n",
    "# target_critic.load_state_dict(critic_state_dict)\n",
    "# critic_model.load_state_dict(critic_state_dict)\n",
    "# target_actor.load_state_dict(actor_state_dict)\n",
    "# actor_model.load_state_dict(actor_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_eps = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_loss_history = []\n",
    "actor_loss_history = []\n",
    "rewards_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_model.train()\n",
    "critic_model.train()\n",
    "target_actor.train()\n",
    "target_critic.train()\n",
    "for i in range(max_iters * 100):\n",
    "    action_history = []\n",
    "    action_count = [0, 0, 0]\n",
    "    done, truncated = False, False\n",
    "    curr_state, info = env.reset()\n",
    "    rewards_sum = 0\n",
    "    count_explore_exploit = [0, 0]\n",
    "    while not done and not truncated: \n",
    "        rand_num = np.random.uniform(0, 1)\n",
    "        if rand_num < epsilon:\n",
    "            action = np.random.normal(0.5, 0.3)\n",
    "            action = np.clip(action, 0, 1)\n",
    "            action = int(action * 10) / 10\n",
    "            action_index = action_space.index(action)\n",
    "            count_explore_exploit[0] += 1\n",
    "        else:\n",
    "            action = target_actor(torch.Tensor(curr_state)).item()\n",
    "            action = action + np.random.normal(0, 0.2)\n",
    "            action = np.clip(action, 0, 1)\n",
    "            action = int(action * 10) / 10\n",
    "            action_index = action_space.index(action)\n",
    "            count_explore_exploit[1] += 1\n",
    "\n",
    "        action_history.append(action)\n",
    "\n",
    "        next_state, reward, done, truncated, info = env.step(action_index)\n",
    "        rewards_sum += reward\n",
    "\n",
    "        if action > 0.5:\n",
    "            action_count[2] += 1\n",
    "        elif action < 0.5:\n",
    "            action_count[0] += 1\n",
    "        else:\n",
    "            action_count[1] += 1\n",
    "\n",
    "        quadruple = (curr_state, action, reward, next_state, done, truncated)\n",
    "        buffer.append(quadruple)\n",
    "        if len(buffer) > buffer_size:\n",
    "            buffer.pop(0)\n",
    "        \n",
    "        curr_state = next_state\n",
    "\n",
    "    rewards_history.append(rewards_sum)\n",
    "    \n",
    "    sample_indices = np.random.choice(len(buffer), size=min(400, len(buffer)), replace=False)\n",
    "    train_sample = [buffer[idx] for idx in sample_indices]\n",
    "\n",
    "    epsilon = max(epsilon * eps_decay, min_eps/10)\n",
    "\n",
    "    total_critic_loss = 0\n",
    "    total_actor_loss = 0\n",
    "    for quadrup in train_sample:\n",
    "        curr_state, action, reward, next_state, done, truncated = quadrup\n",
    "\n",
    "        # critic training\n",
    "        with torch.no_grad():\n",
    "            next_action = target_actor(torch.Tensor(next_state)).item()\n",
    "            next_action = int(next_action * 10) / 10\n",
    "            next_q_val = target_critic(torch.tensor([next_action]), torch.Tensor(next_state))\n",
    "            target_q_val = reward*100 + gamma * next_q_val\n",
    "        curr_q_val = critic_model(torch.tensor([action]), torch.Tensor(curr_state))\n",
    "        critic_loss = loss_function(curr_q_val, target_q_val)\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "        total_critic_loss += critic_loss.item()\n",
    "\n",
    "        #actor training\n",
    "        curr_action_pred = actor_model(torch.tensor(curr_state))\n",
    "        curr_q_pred = critic_model(curr_action_pred, torch.Tensor(curr_state))\n",
    "        actor_loss = -1 * curr_q_pred\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        total_actor_loss += actor_loss.item()\n",
    "    \n",
    "    total_critic_loss /= len(train_sample)\n",
    "    critic_loss_history.append(total_critic_loss)\n",
    "    total_actor_loss /= len(train_sample)\n",
    "    actor_loss_history.append(total_actor_loss)\n",
    "\n",
    "    soft_update(target_actor, actor_model, tau)\n",
    "    soft_update(target_critic, critic_model, tau)\n",
    "\n",
    "    # if i % 10 == 0:\n",
    "    #     target_actor.load_state_dict(actor_model.state_dict())\n",
    "    #     target_critic.load_state_dict(critic_model.state_dict())\n",
    "\n",
    "    print(i, \"-> CRITIC LOSS:\", total_critic_loss, \"-> ACTOR LOSS:\", total_actor_loss, \"-> REWARD:\", rewards_sum, \"\\n-> Actions:\", action_history[:20])\n",
    "    print(\"Action Count\", action_count, \"Exploration COUNT\", count_explore_exploit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_actor.state_dict(), 'ddpg_actor_model_weights.pth')\n",
    "torch.save(target_critic.state_dict(), 'ddpg_critic_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(actor_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(critic_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_actor.eval()\n",
    "\n",
    "done, truncated = False, False\n",
    "curr_state, info = env.reset()\n",
    "total_reward = 0\n",
    "action_history = []\n",
    "action_count = [0, 0, 0]\n",
    "while not done and not truncated: \n",
    "    action = target_actor(torch.Tensor(curr_state)).item()\n",
    "    action = action\n",
    "    action = np.clip(action, 0, 1)\n",
    "    action = int(action * 10) / 10\n",
    "    action_index = action_space.index(action)\n",
    "\n",
    "    action_history.append(action)\n",
    "\n",
    "    next_state, reward, done, truncated, info = env.step(action_index)\n",
    "    total_reward += reward\n",
    "\n",
    "    if action > 0.5:\n",
    "        action_count[2] += 1\n",
    "    elif action < 0.5:\n",
    "        action_count[0] += 1\n",
    "    else:\n",
    "        action_count[1] += 1\n",
    "    \n",
    "    curr_state = next_state\n",
    "env.unwrapped.save_for_render(dir = \"render_logs\")\n",
    "print(action_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_reward)\n",
    "print(action_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renderer = Renderer(render_logs_dir=\"render_logs\")\n",
    "# renderer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
