{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from copy import copy\n",
    "from copy import deepcopy\n",
    "import talib\n",
    "from gym_trading_env.renderer import Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nifty50_index_symbol = '^NSEI'\n",
    "\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2019-06-30'\n",
    "# end_date = '2012-06-11'\n",
    "\n",
    "data = yf.download(nifty50_index_symbol, start=start_date, end=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['feature-MACD'], df['feature-MACD_signal'], df['feature-MACD_hist'] = talib.MACD(df['close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "\n",
    "df['feature-RSI'] = talib.RSI(df['close'], timeperiod=14)\n",
    "\n",
    "df['feature-CCI'] = talib.CCI(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "\n",
    "df['feature-ADX'] = talib.ADX(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "\n",
    "df.dropna(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(columns = ['open', 'high', 'low', 'Adj Close', 'volume'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD\",\n",
    "        df = df,\n",
    "        positions = list(np.linspace(0, 1, 11)), # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "        # trading_fees = 0.01/100, # 0.01% per stock buy / sell\n",
    "        # borrow_interest_rate= 0.0003/100, # 0.0003% per timestep \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, output_shape)\n",
    "\n",
    "    def forward(self, state):\n",
    "        out = torch.relu(self.fc1(state))\n",
    "        out = torch.relu(self.fc2(out))\n",
    "        output = torch.softmax(self.out(out), dim = -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_model = DQN(input_shape, 11)\n",
    "target_dqn = deepcopy(dqn_model)\n",
    "optimizer = torch.optim.Adam(list(dqn_model.parameters()), lr = 0.001)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 1000\n",
    "buffer = []\n",
    "max_iters = 100000\n",
    "gamma = 0.9\n",
    "epsilon = 1\n",
    "eps_decay = 0.995\n",
    "min_eps = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_index = env.action_space.sample()\n",
    "position_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_dqn.load_state_dict(torch.load('dqn_model_weights.pth', weights_only=True))\n",
    "# dqn_model.load_state_dict(torch.load('dqn_model_weights.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_history = []\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max_iters):\n",
    "    action_count = [0, 0, 0]\n",
    "    exploration_exploitation = [0, 0]\n",
    "    action_history = []\n",
    "    total_loss = 0\n",
    "    done, truncated = False, False\n",
    "    curr_state, info = env.reset()\n",
    "    total_reward = 0\n",
    "    while not done and not truncated:\n",
    "        random_num = np.random.uniform(0, 1)\n",
    "        if random_num < epsilon:\n",
    "            action_index = env.action_space.sample()\n",
    "            exploration_exploitation[0] += 1\n",
    "        else:\n",
    "            action_index = torch.argmax(target_dqn(torch.Tensor(curr_state))).item()\n",
    "            exploration_exploitation[1] += 1\n",
    "\n",
    "        action_history.append(action_index)\n",
    "\n",
    "        if action_index > 5:\n",
    "            action_count[2] += 1\n",
    "        elif action_index == 5:\n",
    "            action_count[1] += 1\n",
    "        else:\n",
    "            action_count[0] += 1\n",
    "        \n",
    "        next_state, reward, done, truncated, info = env.step(action_index)\n",
    "        total_reward += reward\n",
    "\n",
    "        buffer.append((curr_state, action_index, reward, next_state, done, truncated))\n",
    "\n",
    "        if len(buffer) > buffer_size:\n",
    "            buffer.pop(0)\n",
    "\n",
    "        curr_state = next_state\n",
    "\n",
    "    reward_history.append(total_reward)\n",
    "    sample_indices = np.random.choice(len(buffer), size=min(400, len(buffer)), replace=False)\n",
    "    train_sample = [buffer[idx] for idx in sample_indices]\n",
    "\n",
    "    for quad in train_sample:\n",
    "        curr_state, action_index, reward, next_state, done, truncated = quad\n",
    "        curr_q = dqn_model(torch.Tensor(curr_state))\n",
    "        with torch.no_grad():\n",
    "            target_q_val = reward*100 + gamma * torch.max(target_dqn(torch.Tensor(next_state))).item() if done is False and truncated is False else reward\n",
    "            target_q = deepcopy(curr_q.detach())\n",
    "            target_q[action_index] = torch.tensor(target_q_val)\n",
    "        loss = loss_function(curr_q, target_q)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    total_loss /= len(sample_indices)\n",
    "    loss_history.append(total_loss)\n",
    "\n",
    "    epsilon = max(min_eps, epsilon * eps_decay)\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        target_dqn.load_state_dict(dqn_model.state_dict())\n",
    "    \n",
    "    print(i, \"REWARD: \", total_reward, \"LOSS: \", total_loss, \"portfolio difference: \", info['portfolio_valuation'] - 1000, \"ACTION COUNT: \", action_count)\n",
    "    print(\"EXPL: \", exploration_exploitation, \"ACTION_HISTORY: \", action_history[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_dqn.state_dict(), 'dqn_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done, truncated = False, False\n",
    "curr_state, info = env.reset()\n",
    "total_reward = 0\n",
    "action_history = []\n",
    "action_count = [0, 0, 0]\n",
    "while not done and not truncated:\n",
    "    action_index = torch.argmax(target_dqn(torch.Tensor(curr_state))).item()\n",
    "\n",
    "    action_history.append(action_index)\n",
    "\n",
    "    if action_index > 5:\n",
    "        action_count[2] += 1\n",
    "    elif action_index == 5:\n",
    "        action_count[1] += 1\n",
    "    else:\n",
    "        action_count[0] += 1\n",
    "    \n",
    "    next_state, reward, done, truncated, info = env.step(action_index)\n",
    "    total_reward += reward\n",
    "    curr_state = next_state\n",
    "env.unwrapped.save_for_render(dir = \"render_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_reward)\n",
    "print(action_count)\n",
    "print(action_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renderer = Renderer(render_logs_dir=\"render_logs\")\n",
    "# renderer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
